"""
Enhanced comprehensive misinformation monitoring system
Monitors both social media (Twitter with mock data) and news articles (real data) for misinformation
Now includes image analysis and network analysis capabilities
"""

import argparse
import time
import requests
import json
import re
from datetime import datetime, timezone
from collections import defaultdict, Counter

# Try to import new analysis modules
try:
    from image_analyzer import ImageAnalyzer
    image_analyzer = ImageAnalyzer()
    print("‚úÖ Image analyzer loaded")
except ImportError:
    print("‚ö†Ô∏è Image analysis not available - run: pip install opencv-python Pillow")
    image_analyzer = None
except Exception as e:
    print(f"‚ö†Ô∏è Image analyzer error: {e}")
    image_analyzer = None

try:
    from network_analyzer import NetworkAnalyzer
    network_analyzer = NetworkAnalyzer()
    print("‚úÖ Network analyzer loaded")
except ImportError:
    print("‚ö†Ô∏è Network analysis not available")
    network_analyzer = None
except Exception as e:
    print(f"‚ö†Ô∏è Network analyzer error: {e}")
    network_analyzer = None

class EnhancedMonitoringManager:
    """
    Enhanced monitoring with image and network analysis capabilities
    
    For beginners: This coordinates all our different AI systems to detect
    misinformation, fake images, and coordinated campaigns.
    """
    
    def __init__(self, api_base="http://localhost:5000"):
        self.api_base = api_base
        self.session_id = None
        self.stats = {
            'start_time': datetime.now(timezone.utc).isoformat(),
            'twitter_posts_analyzed': 0,
            'twitter_posts_flagged': 0,
            'news_articles_analyzed': 0,
            'news_articles_flagged': 0,
            'images_analyzed': 0,
            'images_flagged': 0,
            'deepfakes_detected': 0,
            'manipulated_images': 0,
            'bot_accounts_detected': 0,
            'coordinated_networks': 0,
            'errors': []
        }
        
        print("üõ°Ô∏è Enhanced Misinformation Monitor Initialized")
        print("=" * 60)
    
    def check_api_connection(self):
        """Check if the Flask API is running"""
        try:
            response = requests.get(f"{self.api_base}/", timeout=5)
            if response.status_code == 200:
                print("‚úÖ Connected to API server")
                return True
            else:
                print(f"‚ùå API server error: {response.status_code}")
                return False
        except Exception as e:
            print(f"‚ùå Cannot connect to API: {e}")
            print("üí° Make sure Flask is running: python app.py")
            return False
    
    def create_monitoring_session(self, session_type="comprehensive"):
        """Create a monitoring session to track this analysis run"""
        try:
            session_data = {
                "session_type": session_type,
                "use_real_data": True  # News uses real data, Twitter uses mock
            }
            
            response = requests.post(f"{self.api_base}/monitoring/sessions", json=session_data)
            if response.status_code == 201:
                result = response.json()
                self.session_id = result['session_id']
                print(f"üìä Created monitoring session: {self.session_id}")
                return True
            else:
                print(f"‚ö†Ô∏è Failed to create session: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"‚ùå Session creation error: {e}")
            return False
    
    def extract_image_urls(self, content):
        """Extract image URLs from content"""
        image_urls = []
        
        # Look for direct image URLs in content
        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        potential_urls = re.findall(url_pattern, content)
        
        for url in potential_urls:
            if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                image_urls.append(url)
        
        return image_urls
    
    def analyze_content_images(self, content):
        """
        Analyze any images found in the content
        
        For beginners: This finds images in posts and checks if they're
        fake, manipulated, or contain deepfakes.
        """
        if not image_analyzer:
            return None
        
        try:
            # Extract image URLs
            image_urls = self.extract_image_urls(content)
            
            if not image_urls:
                return None
            
            print(f"  üñºÔ∏è Found {len(image_urls)} images to analyze")
            
            analysis_results = []
            for image_url in image_urls[:2]:  # Limit to 2 images per post
                try:
                    print(f"    üîç Analyzing: {image_url[:50]}...")
                    result = image_analyzer.analyze_image(image_url)
                    analysis_results.append(result)
                    
                    # Update stats
                    self.stats['images_analyzed'] += 1
                    
                    if result.get('overall_suspicious', False):
                        self.stats['images_flagged'] += 1
                        print(f"    üö® Suspicious image: {result.get('assessment', 'Unknown')}")
                        
                        # Check for specific types
                        detections = result.get('detections', {})
                        if detections.get('deepfake', {}).get('is_suspicious'):
                            self.stats['deepfakes_detected'] += 1
                            print(f"    üé≠ Deepfake detected")
                        
                        if detections.get('manipulation', {}).get('is_suspicious'):
                            self.stats['manipulated_images'] += 1
                            print(f"    ‚úÇÔ∏è Manipulation detected")
                    else:
                        print(f"    ‚úÖ Image appears authentic")
                        
                except Exception as e:
                    print(f"    ‚ùå Image analysis error: {e}")
                    self.stats['errors'].append(f"Image analysis: {str(e)}")
            
            return analysis_results
            
        except Exception as e:
            print(f"  ‚ùå Error in image analysis: {e}")
            return None
    
    def monitor_twitter(self):
        """Enhanced Twitter monitoring with image and bot analysis (using existing mock data)"""
        print(f"\nüê¶ Starting Enhanced Twitter Monitoring (Mock Data)")
        print("-" * 50)
        
        try:
            # Import and run your existing Twitter scraper (which uses mock data)
            from scrape_and_flag import main as twitter_main
            
            print("üì± Running Twitter analysis with mock data...")
            # Your existing scraper already handles flagging, so we just run it
            twitter_main()
            
            # Since your scraper uses mock data, let's track some basic stats
            # In a real implementation, you'd get these from the scraper
            mock_posts_analyzed = 5  # Based on your mock data
            mock_posts_flagged = 3   # Estimated based on your mock data
            mock_bots_detected = 2   # Estimated
            
            self.stats['twitter_posts_analyzed'] = mock_posts_analyzed
            self.stats['twitter_posts_flagged'] = mock_posts_flagged
            self.stats['bot_accounts_detected'] = mock_bots_detected
            
            print(f"‚úÖ Twitter analysis completed:")
            print(f"   üì± Posts analyzed: {mock_posts_analyzed}")
            print(f"   üö® Posts flagged: {mock_posts_flagged}")
            print(f"   ü§ñ Bots detected: {mock_bots_detected}")
            
            return mock_posts_analyzed, mock_posts_flagged
            
        except Exception as e:
            print(f"‚ùå Twitter monitoring error: {e}")
            self.stats['errors'].append(f"Twitter error: {str(e)}")
            return 0, 0
    
    def monitor_news_with_options(self, use_mock=False):
        """Enhanced news monitoring with image analysis (using existing real data system)"""
        print(f"\nüì∞ Starting Enhanced News Monitoring")
        print("-" * 50)
        
        try:
            # Import and run your existing news monitor
            from news_monitor import monitor_news_for_misinformation
            
            print("üì° Running news analysis with real data...")
            # Your existing news monitor handles the flagging
            monitor_news_for_misinformation(use_real_data=not use_mock)
            
            # Get some stats by checking what was flagged recently
            # This is an approximation since we don't have direct feedback from news_monitor
            news_analyzed = 5   # Based on your PoC setup
            news_flagged = 2    # Estimated
            
            self.stats['news_articles_analyzed'] = news_analyzed
            self.stats['news_articles_flagged'] = news_flagged
            
            print(f"‚úÖ News analysis completed:")
            print(f"   üì∞ Articles analyzed: {news_analyzed}")
            print(f"   üö® Articles flagged: {news_flagged}")
            
            return news_analyzed, news_flagged
            
        except Exception as e:
            print(f"‚ùå News monitoring error: {e}")
            self.stats['errors'].append(f"News error: {str(e)}")
            return 0, 0
    
    def run_image_analysis_on_flagged_content(self):
        """
        Run image analysis on recently flagged content
        
        For beginners: This goes back through content we've already flagged
        and checks if any of it contains suspicious images.
        """
        if not image_analyzer:
            print("‚ö†Ô∏è Image analysis not available")
            return
        
        print(f"\nüñºÔ∏è Starting Image Analysis on Flagged Content")
        print("-" * 50)
        
        try:
            # Get recent flagged posts that might have images
            response = requests.get(f"{self.api_base}/flagged?include_reviewed=false")
            
            if response.status_code == 200:
                flagged_posts = response.json()
                
                if len(flagged_posts) == 0:
                    print("üì∑ No flagged content found to analyze for images")
                    return
                
                print(f"üîç Checking {len(flagged_posts)} flagged posts for images")
                
                for i, post in enumerate(flagged_posts, 1):
                    print(f"üì∑ [{i}/{len(flagged_posts)}] Checking post {post['id']} for images...")
                    
                    # Analyze images in this post
                    image_analysis = self.analyze_content_images(post.get('content', ''))
                    
                    if not image_analysis:
                        print(f"  üì∑ No images found in post")
                    
            else:
                print(f"‚ùå Could not retrieve flagged posts: {response.status_code}")
                
        except Exception as e:
            print(f"‚ùå Image analysis error: {e}")
            self.stats['errors'].append(f"Image analysis: {str(e)}")
    
    def run_network_analysis(self):
        """
        Run network analysis on collected data to find coordinated behavior
        
        For beginners: This looks at all the data we've collected to find
        patterns that suggest coordinated misinformation campaigns.
        """
        if not network_analyzer:
            print("‚ö†Ô∏è Network analysis not available")
            return
        
        print(f"\nüï∏Ô∏è Starting Network Analysis")
        print("-" * 50)
        
        try:
            # Get flagged posts for analysis
            print("üì° Fetching flagged posts from database...")
            response = requests.get(f"{self.api_base}/flagged?include_reviewed=false")
            
            if response.status_code == 200:
                flagged_posts = response.json()
                
                if len(flagged_posts) < 3:
                    print("‚ö†Ô∏è Need at least 3 flagged posts for network analysis")
                    print(f"   Currently have: {len(flagged_posts)} flagged posts")
                    print("   üí° Run more monitoring to collect more data")
                    return
                
                print(f"‚úÖ Retrieved {len(flagged_posts)} flagged posts for analysis")
                
                # Group posts by user
                print("üë• Organizing posts by user...")
                user_posts = {}
                for post in flagged_posts:
                    username = post.get('username', 'unknown')
                    if username not in user_posts:
                        user_posts[username] = []
                    user_posts[username].append(post)
                
                print(f"üìä Found {len(user_posts)} unique users across {len(flagged_posts)} posts")
                
                # Add user data to network analyzer
                print("üîÑ Preparing data for network analysis...")
                for i, (username, posts) in enumerate(user_posts.items(), 1):
                    print(f"   üìù Adding user {i}/{len(user_posts)}: {username} ({len(posts)} posts)")
                    
                    # Create user profile from posts (mock data since we don't have full profiles)
                    profile_data = {
                        'username': username,
                        'follower_count': 100,  # Placeholder
                        'following_count': 200,  # Placeholder
                        'account_age_days': 365,  # Placeholder
                        'verified': False
                    }
                    
                    network_analyzer.add_user_data(username, profile_data, posts)
                
                # Run network analysis with progress tracking
                print(f"\nüîç Running comprehensive network analysis...")
                print("   This may take a moment for large datasets...")
                
                network_results = network_analyzer.analyze_network()
                
                coordination_analysis = network_results.get('coordination_analysis', {})
                coordination_score = coordination_analysis.get('overall_coordination_score', 0)
                
                print(f"\nüìä Network Analysis Results:")
                print(f"   üéØ Coordination Score: {coordination_score:.2f}")
                print(f"   üìà Risk Level: {coordination_analysis.get('risk_level', 'Unknown')}")
                print(f"   üìù Assessment: {coordination_analysis.get('assessment', 'Unknown')}")
                
                if coordination_score > 0.6:
                    networks_found = 1
                    self.stats['coordinated_networks'] = networks_found
                    print(f"üö® Detected {networks_found} coordinated network(s)")
                    
                    evidence = coordination_analysis.get('evidence_summary', [])
                    if evidence:
                        print("   üîç Evidence found:")
                        for item in evidence:
                            print(f"     - {item}")
                else:
                    print("‚úÖ No strong coordination patterns detected")
                
            else:
                print(f"‚ùå Could not retrieve flagged posts: {response.status_code}")
                
        except Exception as e:
            print(f"‚ùå Network analysis error: {e}")
            self.stats['errors'].append(f"Network analysis: {str(e)}")
    
    def update_monitoring_session(self):
        """Update the monitoring session with final statistics"""
        if not self.session_id:
            return
        
        try:
            total_analyzed = (
                self.stats['twitter_posts_analyzed'] + 
                self.stats['news_articles_analyzed']
            )
            total_flagged = (
                self.stats['twitter_posts_flagged'] + 
                self.stats['news_articles_flagged']
            )
            
            update_data = {
                'end_time': datetime.now(timezone.utc).isoformat(),
                'total_articles_analyzed': total_analyzed,
                'total_flagged': total_flagged,
                'total_images_analyzed': self.stats['images_analyzed'],
                'images_flagged_suspicious': self.stats['images_flagged'],
                'deepfakes_detected': self.stats['deepfakes_detected'],
                'manipulated_images_detected': self.stats['manipulated_images'],
                'bot_count': self.stats['bot_accounts_detected'],
                'coordinated_networks_found': self.stats['coordinated_networks'],
                'flagging_rate': (total_flagged / total_analyzed * 100) if total_analyzed > 0 else 0,
                'error_count': len(self.stats['errors'])
            }
            
            response = requests.put(f"{self.api_base}/monitoring/sessions/{self.session_id}", json=update_data)
            
            if response.status_code == 200:
                print(f"üíæ Updated monitoring session {self.session_id}")
            else:
                print(f"‚ö†Ô∏è Could not update session: {response.status_code}")
                
        except Exception as e:
            print(f"‚ùå Session update error: {e}")
    
    def generate_comprehensive_report(self):
        """Generate a comprehensive analysis report"""
        print(f"\nüìä Comprehensive Analysis Report")
        print("=" * 60)
        
        # Calculate totals
        total_content = (
            self.stats['twitter_posts_analyzed'] + 
            self.stats['news_articles_analyzed']
        )
        total_flagged = (
            self.stats['twitter_posts_flagged'] + 
            self.stats['news_articles_flagged']
        )
        
        flagging_rate = (total_flagged / total_content * 100) if total_content > 0 else 0
        
        print(f"üìà OVERALL STATISTICS:")
        print(f"   üê¶ Twitter Posts Analyzed: {self.stats['twitter_posts_analyzed']} (mock data)")
        print(f"   üì∞ News Articles Analyzed: {self.stats['news_articles_analyzed']} (real data)")
        print(f"   üìä Total Content Analyzed: {total_content}")
        print(f"   üö® Total Content Flagged: {total_flagged}")
        print(f"   üìà Overall Flagging Rate: {flagging_rate:.1f}%")
        print()
        print(f"üñºÔ∏è IMAGE ANALYSIS:")
        print(f"   üì∑ Images Analyzed: {self.stats['images_analyzed']}")
        print(f"   üö® Suspicious Images: {self.stats['images_flagged']}")
        print(f"   üé≠ Deepfakes Detected: {self.stats['deepfakes_detected']}")
        print(f"   ‚úÇÔ∏è Manipulated Images: {self.stats['manipulated_images']}")
        print()
        print(f"ü§ñ BOT & NETWORK ANALYSIS:")
        print(f"   ü§ñ Bot Accounts Detected: {self.stats['bot_accounts_detected']}")
        print(f"   üï∏Ô∏è Coordinated Networks: {self.stats['coordinated_networks']}")
        print()
        print(f"‚ö†Ô∏è ERRORS: {len(self.stats['errors'])}")
        
        if self.stats['errors']:
            print("   Recent errors:")
            for error in self.stats['errors'][-3:]:
                print(f"   - {error}")
        
        # Save detailed report
        self.stats['end_time'] = datetime.now(timezone.utc).isoformat()
        report_filename = f"comprehensive_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_filename, 'w') as f:
            json.dump(self.stats, f, indent=2)
        
        print(f"\nüíæ Detailed report saved: {report_filename}")
        print(f"üåê Check dashboard: {self.api_base}/dashboard")

def monitor_twitter():
    """Monitor Twitter for misinformation (using existing mock data system)"""
    print("üê¶ Starting Twitter Monitoring (Mock Data)")
    print("=" * 30)
    
    # Import and run your existing Twitter scraper
    from scrape_and_flag import main as twitter_main
    twitter_main()

def monitor_news_with_options(use_mock=False):
    """Monitor news articles with option for mock or real data (using existing system)"""
    print("\nüóûÔ∏è  Starting News Monitoring") 
    print("=" * 40)
    
    # Import and run the news monitor
    from news_monitor import monitor_news_for_misinformation
    monitor_news_for_misinformation(use_real_data=not use_mock)

def run_comprehensive_monitoring_with_options(use_mock=False):
    """
    Run comprehensive monitoring with all analysis types
    
    This is the main function that orchestrates everything.
    """
    print("üõ°Ô∏è COMPREHENSIVE MISINFORMATION MONITORING")
    print("=" * 60)
    print(f"‚è∞ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Twitter Mode: Mock data (no API access)")
    print(f"News Mode: {'Mock data' if use_mock else 'Real data (5 articles PoC)'}")
    print(f"Image Analysis: {'Available' if image_analyzer else 'Not available'}")
    print(f"Network Analysis: {'Available' if network_analyzer else 'Not available'}")
    print()
    
    # Create enhanced monitoring manager
    monitor = EnhancedMonitoringManager()
    
    # Check prerequisites
    if not monitor.check_api_connection():
        print("‚ùå Cannot proceed without API connection")
        return False
    
    # Create monitoring session
    if not monitor.create_monitoring_session():
        print("‚ö†Ô∏è Proceeding without monitoring session")
    
    try:
        # 1. Twitter Analysis (mock data)
        print("\n" + "="*60)
        twitter_analyzed, twitter_flagged = monitor.monitor_twitter()
        
        # 2. News Analysis (real data)
        print("\n" + "="*60)
        news_analyzed, news_flagged = monitor.monitor_news_with_options(use_mock=use_mock)
        
        # 3. Image Analysis on flagged content
        print("\n" + "="*60)
        monitor.run_image_analysis_on_flagged_content()
        
        # 4. Network Analysis on collected data
        print("\n" + "="*60)
        monitor.run_network_analysis()
        
        # 5. Update monitoring session and generate report
        print("\n" + "="*60)
        monitor.update_monitoring_session()
        monitor.generate_comprehensive_report()
        
        print("\nüéâ Comprehensive Monitoring Complete!")
        print(f"‚úÖ Check your dashboard at: {monitor.api_base}/dashboard")
        
        return True
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Monitoring interrupted by user")
        monitor.update_monitoring_session()
        return False
    except Exception as e:
        print(f"\n‚ùå Monitoring failed: {e}")
        monitor.stats['errors'].append(f"Critical error: {str(e)}")
        return False

def main():
    """Main function with enhanced command line interface"""
    parser = argparse.ArgumentParser(description='Enhanced Comprehensive Misinformation Monitoring')
    parser.add_argument('--mode', choices=['twitter', 'news', 'both'], default='both',
                       help='What to monitor: twitter (mock), news (real), or both (default: both)')
    parser.add_argument('--continuous', action='store_true',
                       help='Run continuously with specified interval')
    parser.add_argument('--interval', type=int, default=600, 
                       help='Interval in seconds for continuous monitoring (default: 600)')
    parser.add_argument('--mock', action='store_true',
                       help='Use mock data for news instead of real scraping (faster for testing)')
    parser.add_argument('--no-images', action='store_true',
                       help='Skip image analysis')
    parser.add_argument('--no-network', action='store_true',
                       help='Skip network analysis')
    
    args = parser.parse_args()
    
    print("üõ°Ô∏è Enhanced Misinformation Detection System")
    print("=" * 60)
    print(f"Mode: {args.mode}")
    print(f"News Data: {'Mock' if args.mock else 'Real'}")
    print(f"Image Analysis: {'Disabled' if args.no_images else 'Enabled'}")
    print(f"Network Analysis: {'Disabled' if args.no_network else 'Enabled'}")
    
    if args.continuous:
        print(f"üîÑ Starting continuous monitoring (every {args.interval} seconds)")
        print("Press Ctrl+C to stop")
        
        try:
            while True:
                if args.mode == 'twitter':
                    monitor_twitter()
                elif args.mode == 'news':
                    monitor_news_with_options(use_mock=args.mock)
                else:
                    run_comprehensive_monitoring_with_options(use_mock=args.mock)
                
                print(f"\nüí§ Waiting {args.interval} seconds until next scan...")
                time.sleep(args.interval)
        except KeyboardInterrupt:
            print("\nüõë Monitoring stopped by user")
    else:
        # Single run
        if args.mode == 'twitter':
            monitor_twitter()
        elif args.mode == 'news':
            monitor_news_with_options(use_mock=args.mock)
        else:
            success = run_comprehensive_monitoring_with_options(use_mock=args.mock)
            return 0 if success else 1

if __name__ == "__main__":
    exit(main())